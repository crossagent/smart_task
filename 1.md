# 开发指南：Smart Task Agent

本指南专为 **Smart Task Agent** 项目编写，旨在帮助开发者理解系统架构、掌握开发流程，并进行有效的评估与测试。

## 1. 项目概览 (Project Overview)

**Smart Task Agent** 是一个集成 Notion 的智能任务管理助手。它旨在解决任务管理中上下文缺失、信息割裂和手动汇报繁琐的问题。

*   **核心目标**: 将任务创建、进度查询和工作总结转化为与 AI 的自然对话。
*   **关键特性**:
    *   **上下文感知**: 深度理解本地项目代码和文档。
    *   **智能推断**: 自动补充任务细节（背景、目标、优先级等）。
    *   **Notion 集成**: 无缝同步任务状态，自动生成工作报告。

## 2. 系统架构 (System Architecture)

本项目基于 **Google ADK (Agent Development Kit)** 构建，采用多智能体协作架构。

### 2.1 核心工作流: `AddTaskWorkflow`

目前主要实现的 `add_task` 工作流由以下几个关键 Agent 顺序协作完成：

1.  **IntakeRouter (摄入路由)**:
    *   **职责**: 初步解析用户输入，提取基本信息，决定后续处理流程。
    *   **输入**: 用户自然语言指令。
    *   **输出**: 结构化的初步意图和数据。

2.  **SchemaScanner (Schema 扫描)**:
    *   **职责**: 对照核心数据模型（Project/Task），扫描当前已收集的数据，识别缺失的必填字段。
    *   **输出**: 缺失字段列表 (`missing_fields`)。

3.  **InferenceOrchestrator (推断编排)**:
    *   **职责**: **仅在有缺失字段时触发**。协调多个并行 Agent 从上下文中推断缺失信息。
    *   **子 Agents**:
        *   `ProjectSuggester`: 推断所属项目。
        *   `DueDateEstimator`: 推断截止日期。
        *   `PrioritySuggester`: 推断优先级。
    *   **输出**: 填充了推断值的字段建议。

4.  **ClarificationSynthesizer (澄清综合)**:
    *   **职责**: 汇总推断结果，判断置信度。对于低置信度或无法推断的字段，生成友好的澄清问题。
    *   **输出**: 澄清问题列表或确认无需澄清。

5.  **Fulfillment (执行)**:
    *   **职责**: 最终执行动作。如果信息完整且用户确认，则调用 Notion API 创建任务；否则向用户展示澄清问题。

### 2.2 目录结构

```
agents/
  agents/
    clarification.py    # 澄清逻辑
    fulfillment.py      # 执行逻辑 (Notion API 调用)
    intake.py           # 输入处理
    scanner.py          # 字段扫描
    inference/          # 推断相关 Agents
      orchestrator.py
      priority.py
      project.py
      time.py
  workflows/
    add_task.py         # 工作流定义
  dispatcher_agent.py   # 入口分发
  main.py               # 启动脚本
```

## 3. 开发注意事项 (Development Guidelines)

### 3.1 Agent 开发原则
*   **单一职责**: 每个 Agent 应只专注于解决一个小问题（如只负责推断优先级）。
*   **结构化输出**: 尽量让 Agent 输出 Pydantic Model 或 JSON，便于下游 Agent 处理。
*   **无状态设计**: Agent 本身不应存储会话状态，状态应通过 `session.state` 传递。

### 3.2 Notion 集成
*   所有与 Notion 的交互应封装在 `Fulfillment` Agent 或独立的 Tool 中。
*   **不要** 在推断阶段直接调用写操作 API。

## 4. 评估与测试 (Evaluation & Testing)

评估是保证 Agent 智能程度和稳定性的关键环节。我们采用分层评估策略。

### 4.1 单元测试 (Unit Eval) - 针对单个 Agent
**目标**: 验证单个 Agent 在特定输入下是否能输出预期的结果（JSON 结构、字段值等）。

*   **方法**: 为每个 Agent 编写独立的 `.evalset.json` 测试集。
*   **运行命令**:
    ```bash
    adk eval agents/agents/inference/priority.py tests/priority.evalset.json
    ```
*   **关注点**:
    *   Prompt 是否能引导模型输出正确的 JSON 格式？
    *   在边界情况下（如信息不足），Agent 是否能正确返回 "unknown" 或默认值？

### 4.2 集成测试 (Integration Eval) - 针对完整工作流
**目标**: 验证 `AddTaskWorkflow` 从用户输入到最终执行的完整链路。

*   **方法**: 编写包含多轮对话的 `.evalset.json`。
*   **运行命令**:
    ```bash
    adk eval agents/workflows/add_task.py tests/workflow.evalset.json
    ```
*   **关注点**:
    *   Agent 之间的状态传递是否正确（如 `scan_result` 是否正确传递给了 `InferenceOrchestrator`）？
    *   条件执行逻辑是否生效（如字段齐全时是否跳过了推断阶段）？

### 4.3 评估技巧 (Eval Tips)

#### 使用 Rubrics 进行模糊匹配
对于自然语言生成的内容（如澄清问题），不要使用精确字符串匹配。使用 ADK 的 **Rubric** 功能，让 LLM 作为裁判来打分。

**示例配置 (`eval_config.json`)**:
```json
{
  "criteria": {
    "rubric_based_tool_use_quality_v1": {
      "threshold": 1.0,
      "rubrics": [
        {
          "rubricId": "check_clarification",
          "rubricContent": {
            "textProperty": "The response should politely ask for the missing 'due_date'."
          }
        }
      ]
    }
  }
}
```

#### 调试 Trace
使用 `adk web` 查看详细的执行 Trace，这对于理解多 Agent 协作中的数据流向非常有帮助。

```bash
adk web .
```

## 5. 常用命令速查

*   **启动 Web 调试界面**: `adk web .`
*   **运行 CLI 交互**: `adk run .`
*   **运行评估**: `adk eval <agent_path> <evalset_path>`
